# TextToEmotionalSpeech_UG_Project

*Short Intro of how I started working on this project*

During my final year in my Under graduation, I was so interested in making the computers and machines behave like humans. So as a starter I along with my friend started working on this project on December,2017 and finished the project on February,2018 (a period of 3 months roughly) with lots of fun and enjoyment. I had all the codes, documents and other files related to the project in my hard disk. Now I am uploading everything in Github to share with you all so that you guys can enjoy and have fun looking at my cool fun simple project.

Abstract

During Expressive speech, the voice conveys intended
message as well as basic emotions of the speaker. This work focuses on the
development of expressive Text-to-Speech synthesis techniques. Text to
emotional speech conversions involves two processes. The first is to infer
emotions from the information in the text. For this there are many clues in the
word choice, word length, sentence length, and punctuation. The second process
is to speak and express these emotions to our audience. The first process of
finding emotions from the text is often time consuming because it mostly
involved finding emotions based on words used in the input text and hence
training with the huge data set in the existing machine learning algorithms. In
this paper the time complexity of identifying the emotion based on words has
been reduced by identifying emoticons. The second process of speaking with
emotion can be easily done by just adjusting the prosodic variables. Mainly
identified prosodic variables are pitch of the voice, phoneme duration, length of
sounds, loudness, or prominence and timbre (quality of sound). These prosodic
variables which are derived from emotional speech samples are compared with
the features derived from neutral speech.TTS synthesizer computes the values
of a set of prosodic variables, starting from the linguistic information contained
in the text that has to be synthesized. These computed values are adjusted to
produce the intended emotional speech. The new emotional prosodic modules
are still based on the “Classification And Regression Tree” (CART) theory. The
CART method provides the best emotional speech output if it is trained with a
large-coverage corpus.

Initial look

https://drive.google.com/file/d/1brRhJbcIlz4a9ceLYhMTe6e1A6eP5TS4/view?usp=sharing

Final look

see the video attached in the files section
